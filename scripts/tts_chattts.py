#!/usr/bin/env python3
"""
ä¸­æ–‡é…éŸ³ç”Ÿæˆè„šæœ¬ - ä½¿ç”¨ ChatTTS (æœ¬åœ°è¿è¡Œï¼Œå®Œå…¨å…è´¹)
ç”¨æ³•: python tts_chattts.py <chinese.srt> [output.mp3]

é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (~1GB)
"""

import sys
import os
import tempfile
import subprocess
import pysrt
import torch
import ChatTTS
import torchaudio
import numpy as np

def run_command(cmd, description):
    """æ‰§è¡Œå‘½ä»¤å¹¶åœ¨å¤±è´¥æ—¶é€€å‡º"""
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"âŒ å¤±è´¥: {description}")
        if result.stderr:
            print(result.stderr)
        sys.exit(1)
    return result

def mix_segments_with_timestamps(audio_segments, output_audio, temp_dir):
    """æŒ‰å­—å¹•æ—¶é—´è½´åˆå¹¶éŸ³é¢‘ç‰‡æ®µ"""
    if not audio_segments:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„éŸ³é¢‘ç‰‡æ®µ")
        sys.exit(1)

    filter_lines = []
    mix_inputs = []
    for idx, seg in enumerate(audio_segments):
        delay_ms = max(0, int(seg["start_ms"]))
        filter_lines.append(f"[{idx}:a]adelay={delay_ms}|{delay_ms}[a{idx}]")
        mix_inputs.append(f"[a{idx}]")

    filter_lines.append(
        "".join(mix_inputs)
        + f"amix=inputs={len(audio_segments)}:duration=longest:normalize=0[aout]"
    )

    filter_script = os.path.join(temp_dir, "mix.ffmpeg")
    with open(filter_script, "w") as f:
        f.write(";".join(filter_lines))

    cmd = ["ffmpeg", "-y"]
    for seg in audio_segments:
        cmd.extend(["-i", seg["path"]])
    cmd.extend([
        "-filter_complex_script", filter_script,
        "-map", "[aout]",
        "-c:a", "mp3",
        output_audio,
    ])
    run_command(cmd, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ")

def generate_tts(
    input_srt: str,
    output_audio: str = None,
    seed: int = None,
):
    """ä»ä¸­æ–‡å­—å¹•ç”Ÿæˆé…éŸ³ (ä½¿ç”¨ ChatTTS)"""

    # è¯»å–å­—å¹•
    print(f"ğŸ“– è¯»å–å­—å¹•: {input_srt}")
    subs = pysrt.open(input_srt, encoding='utf-8')
    total = len(subs)
    print(f"   å…± {total} æ¡å­—å¹•")

    # åˆå§‹åŒ– ChatTTS
    print("ğŸ”§ åŠ è½½ ChatTTS æ¨¡å‹ (é¦–æ¬¡éœ€è¦ä¸‹è½½)...")
    chat = ChatTTS.Chat()
    chat.load(compile=False)  # compile=True å¯åŠ é€Ÿä½†é¦–æ¬¡ç¼–è¯‘æ…¢

    # è®¾ç½®è¯´è¯äººç‰¹å¾ (å¯å›ºå®š seed ä¿æŒå£°éŸ³ä¸€è‡´)
    if seed is None:
        seed = 42  # å›ºå®šç§å­ç¡®ä¿å£°éŸ³ä¸€è‡´
    torch.manual_seed(seed)
    spk = chat.sample_random_speaker()
    print(f"   ä½¿ç”¨è¯´è¯äººç§å­: {seed}")

    # å‚æ•°è®¾ç½®
    params_infer = ChatTTS.Chat.InferCodeParams(
        spk_emb=spk,
        temperature=0.3,  # è¾ƒä½æ¸©åº¦æ›´ç¨³å®š
        top_P=0.7,
        top_K=20,
    )
    params_refine = ChatTTS.Chat.RefineTextParams(
        prompt='[oral_2][laugh_0][break_4]',  # å£è¯­åŒ–ï¼Œå°‘ç¬‘å£°ï¼Œé€‚å½“åœé¡¿
    )

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    audio_segments = []

    # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
    texts = []
    sub_info = []
    for i, sub in enumerate(subs, start=1):
        text = sub.text.replace('\n', ' ').strip()
        if text:
            texts.append(text)
            sub_info.append((i, sub))

    # æ‰¹é‡ç”Ÿæˆè¯­éŸ³ (ChatTTS æ”¯æŒæ‰¹é‡å¤„ç†)
    print(f"ğŸ™ï¸ ç”Ÿæˆé…éŸ³ä¸­... (å…± {len(texts)} æ¡)")
    batch_size = 10  # æ¯æ‰¹å¤„ç†æ•°é‡

    for batch_start in range(0, len(texts), batch_size):
        batch_end = min(batch_start + batch_size, len(texts))
        batch_texts = texts[batch_start:batch_end]
        batch_info = sub_info[batch_start:batch_end]

        print(f"   å¤„ç† {batch_start + 1}-{batch_end}/{len(texts)}...")

        try:
            wavs = chat.infer(
                batch_texts,
                params_infer_code=params_infer,
                params_refine_text=params_refine,
            )

            for j, (wav, (idx, sub)) in enumerate(zip(wavs, batch_info)):
                segment_path = os.path.join(temp_dir, f"segment_{idx:04d}.wav")

                # ChatTTS è¾“å‡ºæ˜¯ numpy arrayï¼Œé‡‡æ ·ç‡ 24000
                wav_tensor = torch.from_numpy(wav).unsqueeze(0)
                torchaudio.save(segment_path, wav_tensor, 24000)

                audio_segments.append({
                    "path": segment_path,
                    "start_ms": sub.start.ordinal,
                    "text": batch_texts[j],
                })

        except Exception as e:
            print(f"âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            continue

    if not audio_segments:
        print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•éŸ³é¢‘")
        sys.exit(1)

    # åˆå¹¶éŸ³é¢‘
    if output_audio is None:
        base, _ = os.path.splitext(input_srt)
        output_audio = f"{base}_audio.mp3"

    print("ğŸ”§ åˆå¹¶éŸ³é¢‘ç‰‡æ®µï¼ˆæŒ‰å­—å¹•æ—¶é—´è½´ï¼‰...")
    mix_segments_with_timestamps(audio_segments, output_audio, temp_dir)
    print(f"âœ… é…éŸ³å®Œæˆ: {output_audio}")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)

    return output_audio

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python tts_chattts.py <chinese.srt> [output.mp3] [seed]")
        print("\nå‚æ•°è¯´æ˜:")
        print("  seed: è¯´è¯äººç§å­ï¼Œä¸åŒæ•°å­—äº§ç”Ÿä¸åŒå£°éŸ³ (é»˜è®¤: 42)")
        print("\nç¤ºä¾‹:")
        print("  python tts_chattts.py subtitles_zh.srt output.mp3 42")
        sys.exit(1)

    input_srt = sys.argv[1]
    output_audio = sys.argv[2] if len(sys.argv) > 2 else None
    seed = int(sys.argv[3]) if len(sys.argv) > 3 else 42

    generate_tts(input_srt, output_audio, seed)

if __name__ == "__main__":
    main()
