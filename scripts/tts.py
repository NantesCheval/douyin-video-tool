#!/usr/bin/env python3
"""
ä¸­æ–‡é…éŸ³ç”Ÿæˆè„šæœ¬ - ä½¿ç”¨OpenAI TTSä»ä¸­æ–‡å­—å¹•ç”Ÿæˆé…éŸ³
ç”¨æ³•: python tts.py <chinese.srt> [output.mp3]
"""

import sys
import os
import tempfile
import subprocess
import pysrt
from openai import OpenAI

def run_command(cmd, description):
    """æ‰§è¡Œå‘½ä»¤å¹¶åœ¨å¤±è´¥æ—¶é€€å‡º"""
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"âŒ å¤±è´¥: {description}")
        if result.stderr:
            print(result.stderr)
        sys.exit(1)
    return result

def mix_segments_with_timestamps(audio_segments, output_audio, temp_dir):
    """æŒ‰å­—å¹•æ—¶é—´è½´åˆå¹¶éŸ³é¢‘ç‰‡æ®µ"""
    if not audio_segments:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„éŸ³é¢‘ç‰‡æ®µ")
        sys.exit(1)

    filter_lines = []
    mix_inputs = []
    for idx, seg in enumerate(audio_segments):
        delay_ms = max(0, int(seg["start_ms"]))
        filter_lines.append(f"[{idx}:a]adelay={delay_ms}|{delay_ms}[a{idx}]")
        mix_inputs.append(f"[a{idx}]")

    filter_lines.append(
        "".join(mix_inputs)
        + f"amix=inputs={len(audio_segments)}:duration=longest:normalize=0[aout]"
    )

    filter_script = os.path.join(temp_dir, "mix.ffmpeg")
    with open(filter_script, "w") as f:
        f.write(";".join(filter_lines))

    cmd = ["ffmpeg", "-y"]
    for seg in audio_segments:
        cmd.extend(["-i", seg["path"]])
    cmd.extend([
        "-filter_complex_script", filter_script,
        "-map", "[aout]",
        "-c:a", "mp3",
        output_audio,
    ])
    run_command(cmd, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ")

def generate_tts(input_srt: str, output_audio: str = None, voice: str = "alloy"):
    """ä»ä¸­æ–‡å­—å¹•ç”Ÿæˆé…éŸ³"""

    # æ£€æŸ¥API Key
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)

    client = OpenAI(api_key=api_key)

    # è¯»å–å­—å¹•
    print(f"ğŸ“– è¯»å–å­—å¹•: {input_srt}")
    subs = pysrt.open(input_srt, encoding='utf-8')
    total = len(subs)
    print(f"   å…± {total} æ¡å­—å¹•")
    print(f"   ä½¿ç”¨å£°éŸ³: {voice}")

    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾ç‰‡æ®µ
    temp_dir = tempfile.mkdtemp()
    audio_segments = []

    for i, sub in enumerate(subs):
        text = sub.text.replace('\n', ' ').strip()
        if not text:
            continue

        print(f"ğŸ™ï¸ ç”Ÿæˆé…éŸ³... {i+1}/{total}")

        # è°ƒç”¨OpenAI TTS
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text
        )

        # ä¿å­˜ç‰‡æ®µ
        segment_path = os.path.join(temp_dir, f"segment_{i:04d}.mp3")
        response.stream_to_file(segment_path)

        # è®°å½•æ—¶é—´ä¿¡æ¯
        start_ms = sub.start.ordinal
        audio_segments.append({
            "path": segment_path,
            "start_ms": start_ms,
            "text": text
        })

    # åˆå¹¶éŸ³é¢‘ï¼ˆä½¿ç”¨ffmpegæŒ‰æ—¶é—´è½´å¯¹é½ï¼‰
    if output_audio is None:
        base, _ = os.path.splitext(input_srt)
        output_audio = f"{base}_audio.mp3"

    print("ğŸ”§ åˆå¹¶éŸ³é¢‘ç‰‡æ®µï¼ˆæŒ‰å­—å¹•æ—¶é—´è½´ï¼‰...")
    mix_segments_with_timestamps(audio_segments, output_audio, temp_dir)
    print(f"âœ… é…éŸ³å®Œæˆ: {output_audio}")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)

    return output_audio

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python tts.py <chinese.srt> [output.mp3] [voice]")
        print("å¯ç”¨å£°éŸ³: alloy(é»˜è®¤), echo, fable, onyx, nova, shimmer")
        sys.exit(1)

    input_srt = sys.argv[1]
    output_audio = sys.argv[2] if len(sys.argv) > 2 else None
    voice = sys.argv[3] if len(sys.argv) > 3 else "alloy"

    generate_tts(input_srt, output_audio, voice)
